{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07497c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pywt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# CONSTANTS AND VARIABLES\n",
    "FOLDER_PATH = 'datasets/Mariposas'\n",
    "OUTPUT_FILEPATH = 'final_attempt.csv'\n",
    "NEW_SIZE_IMAGES = (32, 32) # 224x224 pixels\n",
    "NUM_COMPONENTS_PCA = 240 # at least 80% cumulative variance ratio\n",
    "characteristic_vectors = []\n",
    "target_list = []\n",
    "file_list = os.listdir(FOLDER_PATH)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=NUM_COMPONENTS_PCA)\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(FOLDER_PATH, file_name)\n",
    "    image = cv2.imread(file_path)\n",
    "    resized_image = cv2.resize(image, NEW_SIZE_IMAGES)\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    coeffs = pywt.dwt2(gray_image, 'bior1.3')\n",
    "    approx, (horizontal_detail, vertical_detail, diagonal_detail) = coeffs\n",
    "    flattened_coeffs = np.concatenate([approx.flatten(), horizontal_detail.flatten(),\n",
    "                            vertical_detail.flatten(), diagonal_detail.flatten()])\n",
    "    normalized_coeffs = (flattened_coeffs - flattened_coeffs.mean()) / flattened_coeffs.std()\n",
    "    characteristic_vectors.append(normalized_coeffs)\n",
    "    target_list.append(int(file_name[:3]))\n",
    "\n",
    "data_matrix = pd.DataFrame(np.vstack(characteristic_vectors))\n",
    "data_matrix['target'] = target_list\n",
    "data_matrix.to_csv('datasets/final_images.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36b25043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pywt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# CONSTANTS AND VARIABLES\n",
    "FOLDER_PATH = 'datasets/Mariposas'\n",
    "OUTPUT_FILEPATH = 'final_attempt.csv'\n",
    "NEW_SIZE_IMAGES = (32, 32) # 224x224 pixels\n",
    "NUM_COMPONENTS_PCA = 240 # at least 80% cumulative variance ratio\n",
    "characteristic_vectors = []\n",
    "target_list = []\n",
    "file_list = os.listdir(FOLDER_PATH)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=NUM_COMPONENTS_PCA)\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(FOLDER_PATH, file_name)\n",
    "    image = cv2.imread(file_path)\n",
    "    \n",
    "    # Resize the color image\n",
    "    resized_image = cv2.resize(image, NEW_SIZE_IMAGES)\n",
    "    \n",
    "    # Apply Wavelet Transform to each color channel\n",
    "    coeffs = [pywt.dwt2(resized_image[:, :, i], 'bior1.3') for i in range(3)]\n",
    "    flattened_coeffs = np.concatenate([c[0].flatten() for c in coeffs] +\n",
    "                                       [c[1][0].flatten() for c in coeffs] +\n",
    "                                       [c[1][1].flatten() for c in coeffs] +\n",
    "                                       [c[1][2].flatten() for c in coeffs])\n",
    "    \n",
    "    # Normalize the coefficients\n",
    "    normalized_coeffs = (flattened_coeffs - flattened_coeffs.mean()) / flattened_coeffs.std()\n",
    "    \n",
    "    characteristic_vectors.append(normalized_coeffs)\n",
    "    target_list.append(int(file_name[:3]))\n",
    "\n",
    "data_matrix = pd.DataFrame(np.vstack(characteristic_vectors))\n",
    "data_matrix['target'] = target_list\n",
    "data_matrix.to_csv('datasets/final_images.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f35890d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/9e/b8/ed5f794359d05cd0bffb894c6418da87b93016ee17b669d55c45d1bd5d5b/tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/2f/2f/3c84f675931ce3bcbc7e23acbba1e5d7f05ce769adab48322de57a9f5928/tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/d1/93/0f4cf5058095d749d464e4f770d2bf339930e5f3374331f0d2fa6ddfbf28/h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\b40946\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.24.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\b40946\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/2c/98/6f45e1e42993c6b9e607b10a9a103e98e2e73d6fd430d4004adff216a98e/grpcio-1.58.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached grpcio-1.58.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9d/44/5a992cb9d7bf8aaae73bc5adaf721ad08731c9d00c1c17999a8691404b0c/google_auth-2.23.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.23.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.7)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/91/6e/db0e545302bf93b6dbbdc496dd192c7f8e8c3bb1584acba069256d8b51d4/charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\b40946\\appdata\\local\\anaconda3\\envs\\utec_cs_machine_learning\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl (276.6 MB)\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached grpcio-1.58.0-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached google_auth-2.23.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "Installing collected packages: rsa, pyasn1-modules, opt-einsum, markdown, keras, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, astunparse, absl-py, requests, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.14.0\n",
      "    Uninstalling keras-2.14.0:\n",
      "      Successfully uninstalled keras-2.14.0\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 certifi-2023.7.22 charset-normalizer-3.2.0 gast-0.4.0 google-auth-2.23.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.58.0 h5py-3.9.0 keras-2.13.1 markdown-3.4.4 opt-einsum-3.3.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-intel-2.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb64c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# CONSTANTS AND VARIABLES\n",
    "FOLDER_PATH = 'datasets/Mariposas'\n",
    "OUTPUT_FILEPATH = 'final_attempt.csv'\n",
    "NEW_SIZE_IMAGES = (28, 28) # 224x224 pixels\n",
    "NUM_COMPONENTS_PCA = 240 # at least 80% cumulative variance ratio\n",
    "characteristic_vectors = []\n",
    "target_list = []\n",
    "file_list = os.listdir(FOLDER_PATH)\n",
    "\n",
    "# Create an ImageDataGenerator to resize the images\n",
    "datagen = ImageDataGenerator(rescale = 1.0 / 255.0)  # Rescale pixel values to [0, 1]\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(FOLDER_PATH, file_name)\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    i = load_img(file_path, target_size=NEW_SIZE_IMAGES)  # Resize the image\n",
    "    x = img_to_array(i)\n",
    "    x = datagen.standardize(x)  # Standardize pixel values\n",
    "    \n",
    "    # Flatten the image into a 1D array\n",
    "    x = x.flatten()\n",
    "    \n",
    "    characteristic_vectors.append(x)\n",
    "    target_list.append(int(file_name[:3]))\n",
    "\n",
    "    \n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255.0,\n",
    "    rotation_range = 40,          # Random rotation in the range [-40, 40] degrees\n",
    "    width_shift_range = 0.2,      # Random horizontal shift by up to 20% of the image width\n",
    "    height_shift_range = 0.2,     # Random vertical shift by up to 20% of the image height\n",
    "    shear_range = 0.2,            # Shear transformation\n",
    "    zoom_range = 0.2,             # Random zoom\n",
    "    horizontal_flip = True,       # Random horizontal flip\n",
    "    fill_mode = 'nearest'         # Fill mode for pixels outside the boundary\n",
    ")\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(FOLDER_PATH, file_name)\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    i = load_img(file_path, target_size = NEW_SIZE_IMAGES)  # Resize the image\n",
    "    x = img_to_array(i)\n",
    "    x = datagen.random_transform(x)  # Apply random transformations for data augmentation\n",
    "    x = datagen.standardize(x)       # Standardize pixel values\n",
    "    \n",
    "    # Flatten the image into a 1D array\n",
    "    x = x.flatten()\n",
    "    \n",
    "    characteristic_vectors.append(x)\n",
    "    target_list.append(int(file_name[:3]))\n",
    "    \n",
    "data_matrix = pd.DataFrame(np.vstack(characteristic_vectors))\n",
    "data_matrix['target'] = target_list\n",
    "data_matrix.to_csv('datasets/final_images.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27f06a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\b40946\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e949950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2202e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = 'datasets/Mariposas'\n",
    "OUTPUT_FILEPATH = 'final_attempt.csv'\n",
    "NEW_SIZE_IMAGES = (32, 32) # 224x224 pixels\n",
    "NUM_COMPONENTS_PCA = 240 # at least 80% cumulative variance ratio\n",
    "characteristic_vectors = []\n",
    "target_list = []\n",
    "file_list = os.listdir(FOLDER_PATH)\n",
    "\n",
    "for file_name in os.listdir(FOLDER_PATH):\n",
    "    file_path = os.path.join(FOLDER_PATH, file_name)\n",
    "    image_read = cv2.imread(file_path)\n",
    "    image_resized = cv2.resize(image_read, NEW_SIZE_IMAGES).flatten()\n",
    "    image_normalized = image_resized / 255.0\n",
    "    characteristic_vectors.append(image_normalized)\n",
    "    target_list.append(int(file_name[:3]))\n",
    "    \n",
    "    \n",
    "data_matrix = pd.DataFrame(np.vstack(characteristic_vectors))\n",
    "data_matrix['target'] = target_list\n",
    "data_matrix.to_csv('datasets/final_images.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
